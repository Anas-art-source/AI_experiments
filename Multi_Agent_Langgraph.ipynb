{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdR/UduVIWvIVvaA6xHrvo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anas-art-source/AI_experiments/blob/main/Multi_Agent_Langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MqtYHs0sy6nq"
      },
      "outputs": [],
      "source": [
        "# %pip install -U langchain langchain_openai langsmith pandas langchain_experimental matplotlib\n",
        "# !pip install langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "# getpass()\n",
        "\n",
        "def _set_if_undefined(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass(f\"Please provide your {var}\")\n",
        "\n",
        "\n",
        "_set_if_undefined(\"OPENAI_API_KEY\")\n",
        "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
        "_set_if_undefined(\"TAVILY_API_KEY\")\n",
        "\n",
        "# Optional, add tracing in LangSmith\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Multi-agent Collaboration\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgVj6Lhwy8AU",
        "outputId": "deb97cbd-e220-48ff-c462-b186bd21b164"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your OPENAI_API_KEY··········\n",
            "Please provide your LANGCHAIN_API_KEY··········\n",
            "Please provide your TAVILY_API_KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent**"
      ],
      "metadata": {
        "id": "ubDt3NL30aso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    ChatMessage,\n",
        "    FunctionMessage,\n",
        "    HumanMessage,\n",
        ")\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
        "\n",
        "\n",
        "def create_agent(llm, tools, system_message: str):\n",
        "    \"\"\"Create an agent.\"\"\"\n",
        "    functions = [format_tool_to_openai_function(t) for t in tools]\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
        "                \" Use the provided tools to progress towards answering the question.\"\n",
        "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
        "                \" will help where you left off. Execute what you can to make progress.\"\n",
        "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
        "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
        "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        ]\n",
        "    )\n",
        "    prompt = prompt.partial(system_message=system_message)\n",
        "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
        "    return prompt | llm.bind_functions(functions)"
      ],
      "metadata": {
        "id": "5T5QiLTRy8DD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Tools**"
      ],
      "metadata": {
        "id": "pWAT1GUv0fyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from typing import Annotated\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# tavily_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
        "\n",
        "repl = PythonREPL()\n",
        "\n",
        "\n",
        "@tool\n",
        "def python_repl(\n",
        "    code: Annotated[str, \"The python code to execute to generate your chart.\"]\n",
        "):\n",
        "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
        "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
        "    try:\n",
        "        result = repl.run(code)\n",
        "    except BaseException as e:\n",
        "        return f\"Failed to execute. Error: {repr(e)}\"\n",
        "    return f\"Succesfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\""
      ],
      "metadata": {
        "id": "rLvQZ6ePy8Fq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create graph**"
      ],
      "metadata": {
        "id": "CEWZLg301BQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define State**"
      ],
      "metadata": {
        "id": "Wc6E9yiP1EzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "from typing import Annotated, List, Sequence, Tuple, TypedDict, Union\n",
        "\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "# This defines the object that is passed between each node\n",
        "# in the graph. We will create different nodes for each agent and tool\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    sender: str"
      ],
      "metadata": {
        "id": "tWTWbdL_0_fi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Agent Node**"
      ],
      "metadata": {
        "id": "6_3hKM3c1PHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "\n",
        "# Helper function to create a node for a given agent\n",
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state)\n",
        "    # We convert the agent output into a format that is suitable to append to the global state\n",
        "    if isinstance(result, FunctionMessage):\n",
        "        pass\n",
        "    else:\n",
        "        result = HumanMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
        "    return {\n",
        "        \"messages\": [result],\n",
        "        # Since we have a strict workflow, we can\n",
        "        # track the sender so we know who to pass to next.\n",
        "        \"sender\": name,\n",
        "    }\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
        "\n",
        "# Research agent and node\n",
        "research_agent = create_agent(\n",
        "    llm,\n",
        "    [tavily_tool],\n",
        "    system_message=\"You should provide accurate data for the chart generator to use.\",\n",
        ")\n",
        "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
        "\n",
        "# Chart Generator\n",
        "chart_agent = create_agent(\n",
        "    llm,\n",
        "    [python_repl],\n",
        "    system_message=\"Any charts you display will be visible by the user.\",\n",
        ")\n",
        "chart_node = functools.partial(agent_node, agent=chart_agent, name=\"Chart Generator\")"
      ],
      "metadata": {
        "id": "b0a0uaGJ1Noy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VzlT5TRR1Roc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9GWWxd21Rq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Av88POKy1Rta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}