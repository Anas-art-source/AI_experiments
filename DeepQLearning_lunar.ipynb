{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAPWLm61NrWFfKbjUqnoAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anas-art-source/AI_experiments/blob/main/DeepQLearning_lunar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOrbjQdGIkyw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!apt-get update && apt-get install -y xvfb\n",
        "!pip install swig\n",
        "!pip install gym[box2d]==0.23.1 pytorch-lightning==1.6.0 pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ],
      "metadata": {
        "id": "SdFjMjyEIny3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import Tensor, nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "g5B54DQGIn1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ],
      "metadata": {
        "id": "_j77TFk6In37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, n_actions):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, n_actions)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x.float())\n"
      ],
      "metadata": {
        "id": "VGTj_5JIIn6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(state, env, net, epsilon=0.0):\n",
        "  if np.random.random() < epsilon:\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    state = torch.tensor([state]).to(device)\n",
        "    q_values = net(state)\n",
        "    _, action = torch.max(q_values, dim=1)\n",
        "    action = int(action.item())\n",
        "  return action"
      ],
      "metadata": {
        "id": "V1vCXLV8In9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "RYEHUM8dIoAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "\n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ],
      "metadata": {
        "id": "J9XhASRGIoC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_environment(name):\n",
        "  env = gym.make(name)\n",
        "  env = TimeLimit(env, max_episode_steps=400)\n",
        "  env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "xRHFdODGIoFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = create_environment('LunarLander-v2')\n",
        "\n",
        "for episode in range(10):\n",
        "  done = False\n",
        "  env.reset()\n",
        "  while not done:\n",
        "    action = env.action_space.sample()\n",
        "    _, _, done, _ = env.step(action)"
      ],
      "metadata": {
        "id": "cz4nrs7KIvvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_video(episode=0)\n"
      ],
      "metadata": {
        "id": "YZeSXTYOIvxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQLearning(LightningModule):\n",
        "\n",
        "  # Initialize.\n",
        "  def __init__(self, env_name, policy=epsilon_greedy, capacity=100_000,\n",
        "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99,\n",
        "               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.15,\n",
        "               eps_last_episode=100, samples_per_epoch=1_000, sync_rate=10):\n",
        "\n",
        "    super().__init__()\n",
        "    self.env = create_environment(env_name)\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[0]\n",
        "    n_actions = self.env.action_space.n\n",
        "\n",
        "    self.q_net = DQN(hidden_size, obs_size, n_actions)\n",
        "\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "\n",
        "    self.policy = policy\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episode(epsilon=self.hparams.eps_start)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None, epsilon=0.):\n",
        "    state = self.env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      if policy:\n",
        "        action = policy(state, self.env, self.q_net, epsilon=epsilon)\n",
        "      else:\n",
        "        action = self.env.action_space.sample()\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      exp = (state, action, reward, done, next_state)\n",
        "      self.buffer.append(exp)\n",
        "      state = next_state\n",
        "\n",
        "  # Forward.\n",
        "  def forward(self, x):\n",
        "    return self.q_net(x)\n",
        "\n",
        "  # Configure optimizers.\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net_optimizer]\n",
        "\n",
        "  # Create dataloader.\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  # Training step.\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    actions = actions.unsqueeze(1)\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "\n",
        "    state_action_values = self.q_net(states).gather(1, actions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      _, next_actions = self.q_net(next_states).max(dim=1, keepdim=True)\n",
        "      next_action_values = self.target_q_net(next_states).gather(1, next_actions)\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "    expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n",
        "\n",
        "    loss = self.hparams.loss_fn(state_action_values, expected_state_action_values)\n",
        "    self.log('episode/Q-Error', loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  # Training epoch end.\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "\n",
        "    epsilon = max(\n",
        "        self.hparams.eps_end,\n",
        "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "    )\n",
        "\n",
        "    self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "    self.log('episode/Return', self.env.return_queue[-1])\n",
        "\n",
        "    if self.current_epoch % self.hparams.sync_rate == 0:\n",
        "      self.target_q_net.load_state_dict(self.q_net.state_dict())"
      ],
      "metadata": {
        "id": "V7bjG9vZIv0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ],
      "metadata": {
        "id": "U1KNkOsYIv3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algo = DeepQLearning('LunarLander-v2')\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=num_gpus,\n",
        "    max_epochs=1_500\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ],
      "metadata": {
        "id": "6RA4EvvqIoHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_video(episode=1500)\n"
      ],
      "metadata": {
        "id": "s6bHL-w4IoKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAiKkn2RIoMT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}